# Model Parameters
model: 
Resume: False

spec_embed: 'EmbedPatchAttention'
spec_len: 3200
patch_len: 64
spec_mask_len: 50
formula: True
formula_max_padding: 15
smiles_max_padding: 100
d_model: 512
num_heads: 8
layer_num: 4
d_ff: 2048
dropout: 0.1

# Data 
data: /fs-computility/ai4chem/luxinyu.p/PatchBasedSelfAttention/qm9s_raman.pkl
vocab_formula: /fs-computility/ai4chem/luxinyu.p/PatchBasedSelfAttention/example/vocab/vocab_formula.pt
vocab_smiles: /fs-computility/ai4chem/luxinyu.p/PatchBasedSelfAttention/example/vocab/vocab_smiles.pt
split_testdata: True
aug_mode:  # Data Augmentation
- verticalNoise
- SMILES
testset_aug:  # Data Augmentation of test set
aug_num: 2
smi_aug_num: 1
max_shift: 
theta: 0.01
alpha: 5

# Train Parameters
save_path: /fs-computility/ai4chem/luxinyu.p/PatchBasedSelfAttention/runs/qm9s_raman/
batch_size: 128
num_epochs: 100
base_lr: 1.0
warmup_steps: 3000
test_train: False
seed: 3234

earlystop_delta: 1
earlystop_patience: 100
report_step: 40
check_point_step: 20
